{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578760fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (1, 8, 250)\n",
      "WARNING: Detected dead channels: [4]\n",
      "         Removing channels with zero variance...\n",
      "         New shape: (1, 7, 250)\n"
     ]
    },
    {
     "ename": "BrainFlowError",
     "evalue": "INVALID_ARGUMENTS_ERROR:13 unable to apply band stop filter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrainFlowError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m t = np.arange(n_points) / fs  \u001b[38;5;66;03m# time vector (seconds)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Run preprocessing from your pipeline ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m filtered_data, fft_data = \u001b[43mpreprocess_raw_eeg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape = (samples, chans, points)\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowcut\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighcut\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoi3order\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m filtered_data = filtered_data[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- FFT for raw data (manual to compare) ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\greym\\Xavier\\src\\functions.py:278\u001b[39m, in \u001b[36mpreprocess_raw_eeg\u001b[39m\u001b[34m(data, fs, lowcut, highcut, MAX_FREQ, power_hz, coi3order)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data[\u001b[32m0\u001b[39m])):\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[43mDataFilter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_bandstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower_hz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFilterTypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBUTTERWORTH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m         data[sample][channel] = butter_bandpass_filter(data[sample][channel], \u001b[32m2\u001b[39m, \u001b[32m120\u001b[39m, fs, order=\u001b[32m5\u001b[39m)\n\u001b[32m    281\u001b[39m         \u001b[38;5;66;03m# DataFilter.perform_bandstop(data[sample][channel], 250, 10.0, 1.0, 6, FilterTypes.BUTTERWORTH.value, 0)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\greym\\Xavier\\venv\\Lib\\site-packages\\brainflow\\data_filter.py:698\u001b[39m, in \u001b[36mDataFilter.perform_bandstop\u001b[39m\u001b[34m(cls, data, sampling_rate, start_freq, stop_freq, order, filter_type, ripple)\u001b[39m\n\u001b[32m    695\u001b[39m res = DataHandlerDLL.get_instance().perform_bandstop(data, data.shape[\u001b[32m0\u001b[39m], sampling_rate, start_freq,\n\u001b[32m    696\u001b[39m                                                      stop_freq, order, filter_type, ripple)\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res != BrainFlowExitCodes.STATUS_OK.value:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BrainFlowError(\u001b[33m'\u001b[39m\u001b[33munable to apply band stop filter\u001b[39m\u001b[33m'\u001b[39m, res)\n",
      "\u001b[31mBrainFlowError\u001b[39m: INVALID_ARGUMENTS_ERROR:13 unable to apply band stop filter"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import preprocess_raw_eeg\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# --- Load your raw EEG sample ---\n",
    "# Replace this with your own npy file (4x250 or more)\n",
    "raw_data = np.load(r\"C:\\Users\\greym\\Xavier\\datasets\\Grey\\motor\\left\\1763064382661.npy\")  # example path\n",
    "\n",
    "fs = 250  # sampling rate (Hz)\n",
    "n_channels, n_points = raw_data.shape\n",
    "t = np.arange(n_points) / fs  # time vector (seconds)\n",
    "\n",
    "# --- Run preprocessing from your pipeline ---\n",
    "filtered_data, fft_data = preprocess_raw_eeg(\n",
    "    data=raw_data.reshape(1, n_channels, n_points),  # shape = (samples, chans, points)\n",
    "    fs=fs, lowcut=7, highcut=45, coi3order=0\n",
    ")\n",
    "filtered_data = filtered_data[0]  # remove batch dimension\n",
    "\n",
    "# --- FFT for raw data (manual to compare) ---\n",
    "freqs = fftfreq(n_points, 1 / fs)[: n_points // 2]\n",
    "fft_raw = np.abs(fft(raw_data))[:, : n_points // 2]\n",
    "fft_filtered = np.abs(fft(filtered_data))[:, : n_points // 2]\n",
    "\n",
    "# =========================================\n",
    "#        TIME-DOMAIN COMPARISON\n",
    "# =========================================\n",
    "plt.figure(figsize=(12, 8))\n",
    "for ch in range(n_channels):\n",
    "    plt.subplot(n_channels, 1, ch + 1)\n",
    "    plt.plot(t, raw_data[ch], color=\"gray\", alpha=0.6, label=\"Raw EEG\")\n",
    "    plt.plot(t, filtered_data[ch], color=\"blue\", label=\"Filtered EEG\", linewidth=1)\n",
    "    plt.ylabel(f\"Ch {ch+1}\")\n",
    "    plt.grid(True)\n",
    "    if ch == 0:\n",
    "        plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.suptitle(\"EEG Filtering Visualization (Time Domain)\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "#        FREQUENCY-DOMAIN COMPARISON\n",
    "# =========================================\n",
    "plt.figure(figsize=(12, 8))\n",
    "for ch in range(n_channels):\n",
    "    plt.subplot(n_channels, 1, ch + 1)\n",
    "    plt.plot(freqs, fft_raw[ch], color=\"gray\", alpha=0.5, label=\"Raw Spectrum\")\n",
    "    plt.plot(freqs, fft_filtered[ch], color=\"purple\", label=\"Filtered Spectrum\")\n",
    "    plt.xlim(0, 60)\n",
    "    plt.ylabel(f\"Ch {ch+1}\")\n",
    "    plt.grid(True)\n",
    "    if ch == 0:\n",
    "        plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.suptitle(\"EEG Filtering Visualization (Frequency Domain)\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "#        DIFFERENCE TRACE (optional)\n",
    "# =========================================\n",
    "plt.figure(figsize=(12, 6))\n",
    "for ch in range(n_channels):\n",
    "    diff = raw_data[ch] - filtered_data[ch]\n",
    "    plt.plot(t, diff + ch * 50, label=f\"Ch {ch+1}\")  # offset for clarity\n",
    "plt.title(\"Difference (Raw - Filtered)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude offset per channel\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6e8fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (8, 250)\n",
      "\n",
      "=== RAW ADC VALUES (UNSCALED) ===\n",
      "Ch 1: mean=-177.2219, std=1.0963, min=-179.35, max=-175.18\n",
      "Ch 2: mean=-1583.5465, std=1.3084, min=-1586.20, max=-1581.15\n",
      "Ch 3: mean=947.2589, std=2.0823, min=942.92, max=951.51\n",
      "Ch 4: mean=-76.8679, std=1.1253, min=-79.15, max=-74.40\n",
      "Ch 5: mean=4190.9512, std=0.0000, min=4190.95, max=4190.95\n",
      "Ch 6: mean=834.2196, std=5.2898, min=820.86, max=844.25\n",
      "Ch 7: mean=-504.2967, std=1.1679, min=-506.83, max=-501.75\n",
      "Ch 8: mean=-1027.5952, std=1.2478, min=-1029.92, max=-1025.22\n",
      "\n",
      "=== SCALED TO MICROVOLTS (µV) ===\n",
      "Ch 1: mean=-3.96µV, std=0.02µV, min=-4.01µV, max=-3.92µV\n",
      "Ch 2: mean=-35.40µV, std=0.03µV, min=-35.45µV, max=-35.34µV\n",
      "Ch 3: mean=21.17µV, std=0.05µV, min=21.08µV, max=21.27µV\n",
      "Ch 4: mean=-1.72µV, std=0.03µV, min=-1.77µV, max=-1.66µV\n",
      "Ch 5: mean=93.68µV, std=0.00µV, min=93.68µV, max=93.68µV\n",
      "Ch 6: mean=18.65µV, std=0.12µV, min=18.35µV, max=18.87µV\n",
      "Ch 7: mean=-11.27µV, std=0.03µV, min=-11.33µV, max=-11.22µV\n",
      "Ch 8: mean=-22.97µV, std=0.03µV, min=-23.02µV, max=-22.92µV\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "raw_data = np.load(r\"C:\\Users\\greym\\Xavier\\datasets\\Grey\\motor\\left\\1763064382661.npy\")\n",
    "print(\"Raw shape:\", raw_data.shape)\n",
    "print(\"\\n=== RAW ADC VALUES (UNSCALED) ===\")\n",
    "for i, ch in enumerate(raw_data):\n",
    "    print(f\"Ch {i+1}: mean={np.mean(ch):.4f}, std={np.std(ch):.4f}, min={np.min(ch):.2f}, max={np.max(ch):.2f}\")\n",
    "\n",
    "# Apply proper scaling to microvolts\n",
    "SCALE_FACTOR = 0.02235174  # For Cyton: 4.5V / 24 (gain) / (2^23 - 1) * 1000000\n",
    "scaled_data = raw_data * SCALE_FACTOR\n",
    "\n",
    "print(\"\\n=== SCALED TO MICROVOLTS (µV) ===\")\n",
    "for i, ch in enumerate(scaled_data):\n",
    "    print(f\"Ch {i+1}: mean={np.mean(ch):.2f}µV, std={np.std(ch):.2f}µV, min={np.min(ch):.2f}µV, max={np.max(ch):.2f}µV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475d8f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "from brainflow import BoardShim, BoardIds\n",
    "print(BoardShim.get_eeg_channels(BoardIds.CYTON_BOARD))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
